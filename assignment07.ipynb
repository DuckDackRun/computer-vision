{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "Welcome to the assignment for week 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Backpropagation\n",
    "\n",
    "Read the Rojas book (https://www.inf.fu-berlin.de/inst/ag-ki/rojas_home/documents/1996/NeuralNetworks/neuron.pdf), chapter 7.3.3 and learn about the \"matrix way\" of implementing backprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 7.1 XOR\n",
    "\n",
    "* Implement a two-layer artificial neural network with two input neurons and one output neuron. Choose the number of hidden neurons to your liking and add an error \"neuron\" to your network. \n",
    "* Our goal is to learn the XOR function. What does the network return for random weights of all combinations of (binary) inputs? **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# XOR input and output pairs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "target_outputs = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, input_neurons= 2, hidden_neurons= 2, hidden_layer= 1):\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = 1\n",
    "        self.layerWeightsMatrix= []\n",
    "        self.layerBias= []\n",
    "\n",
    "        np.random.seed(42)\n",
    "        for _ in range(hidden_layer):\n",
    "            self.layerWeightsMatrix.append(np.random.uniform(-1, 1, (input_neurons, hidden_neurons)))\n",
    "            self.layerBias.append(np.random.uniform(-1, 1, (1, hidden_neurons)))\n",
    "        \n",
    "        self.layerWeightsMatrix.append(np.random.uniform(-1, 1, (hidden_neurons, self.output_neurons)))\n",
    "        self.layerBias.append(np.random.uniform(-1, 1, (1, self.output_neurons)))\n",
    "        \n",
    "    def predict(self, input):\n",
    "        cur_result = input\n",
    "        \n",
    "        for i, weightMatrix in enumerate(self.layerWeightsMatrix):\n",
    "            cur_result = sigmoid(np.dot(cur_result, weightMatrix) + self.layerBias[i])\n",
    "            \n",
    "        return cur_result\n",
    "    \n",
    "    def getWeightUpdates(self, input, t, learnrate=0.01):\n",
    "        '''should also return Accuracy'''\n",
    "        weightUpdate = []\n",
    "        cur_result = input\n",
    "        d_matrix = []\n",
    "        output_list=[input]\n",
    "        \n",
    "        for i, weightMatrix in enumerate(self.layerWeightsMatrix):\n",
    "            \n",
    "            product = np.dot(cur_result, weightMatrix)\n",
    "            o = product + self.layerBias[i]\n",
    "            cur_result = sigmoid(o)\n",
    "            \n",
    "            diagonal_o = o * (1 - o)\n",
    "            diagonal_matrix = np.diag(diagonal_o)\n",
    "            d_matrix.append(diagonal_matrix)\n",
    "            output_list.append(o)\n",
    "            \n",
    "        predicted_output = cur_result\n",
    "        mse_error = (predicted_output - t)**2/2\n",
    "        \n",
    "        cur_backpropagated_error = mse_error\n",
    "        for i in range(0, len(self.layerBias),-1):\n",
    "            # update backpropagated errors, delta2\n",
    "            cur_backpropagated_error = d_matrix[i]*cur_backpropagated_error\n",
    "            \n",
    "            updateWeightMatrix = - learnrate* cur_backpropagated_error* output_list[i].T\n",
    "            weightUpdate.append(updateWeightMatrix)\n",
    "            \n",
    "            cur_backpropagated_error= self.layerWeightsMatrix[i].T @ cur_backpropagated_error\n",
    "    # need to store local diff\n",
    "    \n",
    "model = NN()\n",
    "predicted_output = model.predict(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0] -> Predicted Output: 0.5378, Error: -0.5378\n",
      "Input: [0 1] -> Predicted Output: 0.5219, Error: 0.4781\n",
      "Input: [1 0] -> Predicted Output: 0.5888, Error: 0.4112\n",
      "Input: [1 1] -> Predicted Output: 0.5753, Error: -0.5753\n"
     ]
    }
   ],
   "source": [
    "# just compute error instead of error neuron\n",
    "errors = target_outputs - predicted_output\n",
    "\n",
    "# Display results\n",
    "for i in range(len(inputs)):\n",
    "    print(f\"Input: {inputs[i]} -> Predicted Output: {predicted_output[i][0]:.4f}, Error: {errors[i][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.2: Backpropagation\n",
    "\n",
    "Implement Backpropagation and optimize the weights of your neural network using the XOR training set: \n",
    "\n",
    "| x | y |\n",
    "| -------- | ------- |\n",
    "| (0,0) | 0 |\n",
    "| (0,1) | 1 |\n",
    "| (1,0) | 1 |\n",
    "| (1,1) | 0 |\n",
    "\n",
    "* How many training iterations do you need? Plot the network error over the number of iterations! **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52190923]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.3: MNIST (BONUS)\n",
    "\n",
    "* Train your network on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) and state the model accuracy (or the model error) for the training and test sets. **(RESULT)** \n",
    "* Compare to this [list](https://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354) and report on the performance of your model. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access MNIST using torchvision (https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html)\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
